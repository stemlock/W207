{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods: Tree Bagging; Random Forests; Adaboost from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import mode \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "\n",
    "# For producing decision tree diagrams.\n",
    "from IPython.core.display import Image, display\n",
    "from six import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leverage the *boston housing data set* to try out ensemble methods. First, make the output binary for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the boston housing data\n",
    "boston = load_boston()\n",
    "X, Y = boston.data, boston.target\n",
    "\n",
    "# binarize the output so it is now a classification task\n",
    "Y = (Y > np.median(Y)).astype(int)\n",
    "\n",
    "# Shuffle the data, but make sure that the features and accompanying labels stay in sync.\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "\n",
    "# Split into train and test.\n",
    "train_data, train_labels = X[:350], Y[:350]\n",
    "test_data, test_labels = X[350:], Y[350:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to print out the pseudocode version of the tree as an alternative to GraphViz, of which a graphic will also be produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurse(left, right, threshold, features, value, node):\n",
    "        if (threshold[node] != -2):\n",
    "                print (\"if ( \" + features[node] + \" <= \" + str(threshold[node]) + \" ) {\")\n",
    "                if left[node] != -1:\n",
    "                        recurse (left, right, threshold, features, value, left[node])\n",
    "                print (\"} else {\")\n",
    "                if right[node] != -1:\n",
    "                        recurse (left, right, threshold, features,value, right[node])\n",
    "                print (\"}\")\n",
    "        else:\n",
    "                print (\"return \" + str(value[node]))\n",
    "\n",
    "def get_code(tree, feature_names):\n",
    "        left      = tree.tree_.children_left\n",
    "        right     = tree.tree_.children_right\n",
    "        threshold = tree.tree_.threshold\n",
    "        features  = [feature_names[i] for i in tree.tree_.feature]\n",
    "        value = tree.tree_.value\n",
    "\n",
    "        recurse(left, right, threshold, features, value, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if ( LSTAT <= 7.684999942779541 ) {\n",
      "if ( PTRATIO <= 21.5 ) {\n",
      "return [[ 0. 94.]]\n",
      "} else {\n",
      "return [[1. 0.]]\n",
      "}\n",
      "} else {\n",
      "if ( LSTAT <= 14.045000076293945 ) {\n",
      "if ( RM <= 6.315499782562256 ) {\n",
      "if ( INDUS <= 13.360000133514404 ) {\n",
      "if ( CRIM <= 0.05717500112950802 ) {\n",
      "return [[14.  0.]]\n",
      "} else {\n",
      "if ( CRIM <= 0.5821950137615204 ) {\n",
      "if ( TAX <= 404.5 ) {\n",
      "if ( INDUS <= 4.2200000286102295 ) {\n",
      "return [[0. 4.]]\n",
      "} else {\n",
      "if ( AGE <= 28.049999237060547 ) {\n",
      "return [[0. 3.]]\n",
      "} else {\n",
      "if ( RM <= 5.92300009727478 ) {\n",
      "return [[8. 0.]]\n",
      "} else {\n",
      "if ( AGE <= 45.39999961853027 ) {\n",
      "if ( TAX <= 255.0 ) {\n",
      "return [[1. 0.]]\n",
      "} else {\n",
      "return [[0. 5.]]\n",
      "}\n",
      "} else {\n",
      "if ( B <= 395.9150085449219 ) {\n",
      "if ( NOX <= 0.47050000727176666 ) {\n",
      "return [[4. 0.]]\n",
      "} else {\n",
      "if ( DIS <= 2.746250033378601 ) {\n",
      "return [[2. 0.]]\n",
      "} else {\n",
      "return [[0. 5.]]\n",
      "}\n",
      "}\n",
      "} else {\n",
      "return [[8. 0.]]\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "} else {\n",
      "return [[4. 0.]]\n",
      "}\n",
      "} else {\n",
      "return [[7. 0.]]\n",
      "}\n",
      "}\n",
      "} else {\n",
      "if ( B <= 390.7099914550781 ) {\n",
      "if ( NOX <= 0.6010000109672546 ) {\n",
      "return [[4. 0.]]\n",
      "} else {\n",
      "if ( CRIM <= 3.6875849962234497 ) {\n",
      "if ( B <= 285.4449996948242 ) {\n",
      "return [[0. 1.]]\n",
      "} else {\n",
      "return [[3. 0.]]\n",
      "}\n",
      "} else {\n",
      "return [[0. 6.]]\n",
      "}\n",
      "}\n",
      "} else {\n",
      "return [[0. 6.]]\n",
      "}\n",
      "}\n",
      "} else {\n",
      "if ( TAX <= 305.5 ) {\n",
      "return [[ 0. 19.]]\n",
      "} else {\n",
      "if ( RAD <= 5.5 ) {\n",
      "if ( INDUS <= 14.070000171661377 ) {\n",
      "if ( RM <= 6.700500011444092 ) {\n",
      "return [[4. 0.]]\n",
      "} else {\n",
      "return [[0. 1.]]\n",
      "}\n",
      "} else {\n",
      "return [[0. 2.]]\n",
      "}\n",
      "} else {\n",
      "if ( CRIM <= 5.764664888381958 ) {\n",
      "return [[ 0. 11.]]\n",
      "} else {\n",
      "return [[1. 0.]]\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "} else {\n",
      "if ( PTRATIO <= 19.450000762939453 ) {\n",
      "if ( TAX <= 320.5 ) {\n",
      "if ( CHAS <= 0.5 ) {\n",
      "if ( LSTAT <= 15.775000095367432 ) {\n",
      "if ( INDUS <= 7.389999866485596 ) {\n",
      "return [[1. 0.]]\n",
      "} else {\n",
      "return [[0. 3.]]\n",
      "}\n",
      "} else {\n",
      "if ( RM <= 5.507000207901001 ) {\n",
      "return [[0. 1.]]\n",
      "} else {\n",
      "if ( RM <= 6.066499948501587 ) {\n",
      "return [[8. 0.]]\n",
      "} else {\n",
      "if ( B <= 394.7099914550781 ) {\n",
      "return [[1. 0.]]\n",
      "} else {\n",
      "return [[0. 1.]]\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "} else {\n",
      "return [[0. 5.]]\n",
      "}\n",
      "} else {\n",
      "if ( AGE <= 49.49999809265137 ) {\n",
      "return [[0. 1.]]\n",
      "} else {\n",
      "if ( LSTAT <= 14.21500015258789 ) {\n",
      "if ( PTRATIO <= 16.950000286102295 ) {\n",
      "return [[0. 1.]]\n",
      "} else {\n",
      "return [[1. 0.]]\n",
      "}\n",
      "} else {\n",
      "return [[17.  0.]]\n",
      "}\n",
      "}\n",
      "}\n",
      "} else {\n",
      "if ( RM <= 6.838000059127808 ) {\n",
      "if ( AGE <= 83.0999984741211 ) {\n",
      "if ( AGE <= 82.0999984741211 ) {\n",
      "return [[11.  0.]]\n",
      "} else {\n",
      "return [[0. 1.]]\n",
      "}\n",
      "} else {\n",
      "return [[78.  0.]]\n",
      "}\n",
      "} else {\n",
      "if ( LSTAT <= 18.494999885559082 ) {\n",
      "return [[1. 0.]]\n",
      "} else {\n",
      "return [[0. 1.]]\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create a decision tree and print the output of the decision tree rules\n",
    "dt = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\", random_state=0)\n",
    "dt.fit(train_data, train_labels)\n",
    "get_code(dt, boston.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a visual to be outputted\n",
    "tree.export_graphviz(dt, 'tree.dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, compare the performance of a single decision tree vs that of a random forest and an Adaboost forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (a decision tree): 0.8782051282051282\n",
      "Accuracy (a random forest): 0.9102564102564102\n",
      "Accuracy (adaboost with decision trees): 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "print ('Accuracy (a decision tree):', dt.score(test_data, test_labels))\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=1000)\n",
    "rfc.fit(train_data, train_labels)\n",
    "\n",
    "print ('Accuracy (a random forest):', rfc.score(test_data, test_labels))\n",
    "\n",
    "abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100, learning_rate=0.2)\n",
    "\n",
    "abc.fit(train_data, train_labels)\n",
    "print ('Accuracy (adaboost with decision trees):', abc.score(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the bagging functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "def bootstrap_tree(bs_tree, train_data, train_labels, test_data):\n",
    "    \n",
    "    '''\n",
    "    Function to create a decision tree on a bootstrapped sample and predict on test data.\n",
    "    '''\n",
    "    \n",
    "    # Create a bootstrap sample\n",
    "    bs_sample_idx = np.random.choice(range(train_data.shape[0]), size=train_data.shape[0], replace=True)\n",
    "    \n",
    "    # Create the bootstrap data and labels\n",
    "    bs_data = train_data[bs_sample_idx, :]\n",
    "    bs_labels = train_labels[bs_sample_idx]\n",
    "    \n",
    "    # Train the tree and predict on test_data\n",
    "    bs_tree.fit(bs_data, bs_labels)\n",
    "    bs_tree_preds = bs_tree.predict(test_data)\n",
    "    \n",
    "    return bs_tree_preds\n",
    "\n",
    "def bagging(bs_tree, num_trees, train_data, train_labels, test_data):\n",
    "    \n",
    "    '''\n",
    "    Function to perform bagging using a specified num_trees, where each tree is fit on a bootstrap sample.\n",
    "    '''\n",
    "    \n",
    "    # Initialize the ndarray to hold the predictions per tree\n",
    "    bagged_preds = np.zeros((num_trees, test_data.shape[0]))\n",
    "    \n",
    "    # For each tree, return the predictions and store in the bagged_preds ndarray\n",
    "    for tree in range(num_trees):\n",
    "        bagged_preds[tree] = bootstrap_tree(bs_tree, train_data, train_labels, test_data)\n",
    "      \n",
    "    # Find the most common prediction for each test sample\n",
    "    val, count = mode(bagged_preds, axis = 0) \n",
    "    final_preds = val.ravel().tolist() \n",
    "    \n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for a single tree: 0.8333333333333334\n",
      "Accuracy score for a bagged forest: 0.9038461538461539\n"
     ]
    }
   ],
   "source": [
    "# Set the number of trees and tree model\n",
    "num_trees = 100\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\")\n",
    "\n",
    "# Generate the bagged trees and the majority vote classification for the predictions\n",
    "bagged_trees_preds = bagging(clf, num_trees, train_data, train_labels, test_data)\n",
    "\n",
    "# Create a single bagged tree\n",
    "bs_tree_preds = bootstrap_tree(clf, train_data, train_labels, test_data)\n",
    "\n",
    "# Compare the results\n",
    "print(\"Accuracy score for a single tree:\", sum(bs_tree_preds == test_labels) / len(test_labels))\n",
    "print(\"Accuracy score for a bagged forest:\", sum(bagged_trees_preds == test_labels) / len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests add the twist of subsampling features at each node. Random forests take p' = sqrt(p) features. This can be implemented by updating the DecisionTreeClassifer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for a single tree: 0.8205128205128205\n",
      "Accuracy score for a bagged forest: 0.8910256410256411\n",
      "Accuracy score for a random forest: 0.9102564102564102\n"
     ]
    }
   ],
   "source": [
    "# Set the number of trees and tree models\n",
    "num_trees = 100\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\")\n",
    "random_clf = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\", max_features=\"auto\", random_state=0)\n",
    "\n",
    "# Create a single bagged tree\n",
    "bs_tree_preds = bootstrap_tree(clf, train_data, train_labels, test_data)\n",
    "\n",
    "# Generate the bagged trees and the majority vote classification for the predictions\n",
    "bagged_trees_preds = bagging(clf, num_trees, train_data, train_labels, test_data)\n",
    "\n",
    "# Generate the random forest trees and the majority vote classification for the predictions\n",
    "random_forest_preds = bagging(random_clf, num_trees, train_data, train_labels, test_data)\n",
    "\n",
    "# Compare the results\n",
    "print(\"Accuracy score for a single tree:\", sum(bs_tree_preds == test_labels) / len(test_labels))\n",
    "print(\"Accuracy score for a bagged forest:\", sum(bagged_trees_preds == test_labels) / len(test_labels))\n",
    "print(\"Accuracy score for a random forest:\", sum(random_forest_preds == test_labels) / len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost from Scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a single implementation of Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels into +/- 1\n",
    "train_labels_pm = train_labels * 2 - 1\n",
    "test_labels_pm = test_labels * 2 - 1\n",
    "\n",
    "# Initialize with equal weights on each data point\n",
    "data_weights = np.ones(train_data.shape[0]).astype(\"float\") / float(train_data.shape[0])\n",
    "\n",
    "# Fit the decision tree\n",
    "bdtc = DecisionTreeClassifier(max_depth=1,criterion=\"entropy\", splitter=\"best\")\n",
    "bdtc.fit(train_data, train_labels_pm, sample_weight=data_weights)\n",
    "\n",
    "# Save the bdtc_predictions \n",
    "bdtc_predictions = bdtc.predict(train_data)\n",
    "\n",
    "# Save the bdtc_predictions_test\n",
    "bdtc_predictions_test = bdtc.predict(test_data)\n",
    "\n",
    "# Save the weighted error rate\n",
    "bdtc_weighted_error_rate = np.sum(data_weights * (1 * (bdtc_predictions != train_labels_pm)).astype(\"float\"))\n",
    "\n",
    "# Save the error_rate_alpha\n",
    "error_rate_alpha = np.log((1 - bdtc_weighted_error_rate) / bdtc_weighted_error_rate) / 2\n",
    "    \n",
    "# Reweight the weights on each data point    \n",
    "data_weights_updated = data_weights * np.exp(-1 * error_rate_alpha * bdtc_predictions * train_labels_pm)\n",
    "data_weights_updated = data_weights_updated / sum(data_weights_updated)\n",
    "data_weights = data_weights_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8269230769230769"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdtc.score(test_data, test_labels_pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
