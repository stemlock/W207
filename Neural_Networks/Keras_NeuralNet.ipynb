{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-DM_yDXT6dh"
   },
   "source": [
    "# Part 1: Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 540,
     "status": "ok",
     "timestamp": 1613014243625,
     "user": {
      "displayName": "Todd Holloway",
      "photoUrl": "https://lh3.googleusercontent.com/-Fjjf5JgE-cQ/AAAAAAAAAAI/AAAAAAAARlo/rz8G-eBTNGo/s64/photo.jpg",
      "userId": "06017688276795782893"
     },
     "user_tz": 480
    },
    "id": "JsFAIBL8T6dh",
    "outputId": "d5330807-4384-433c-ec2f-b826cd3a51ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as K\n",
    "\n",
    "np.random.seed(0)\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_P2puejeT6dp"
   },
   "source": [
    "Now for MNIST data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 953,
     "status": "ok",
     "timestamp": 1613014287336,
     "user": {
      "displayName": "Todd Holloway",
      "photoUrl": "https://lh3.googleusercontent.com/-Fjjf5JgE-cQ/AAAAAAAAAAI/AAAAAAAARlo/rz8G-eBTNGo/s64/photo.jpg",
      "userId": "06017688276795782893"
     },
     "user_tz": 480
    },
    "id": "v9NL8Cc_T6dp",
    "outputId": "537b7ec1-a8cb-4862-83d5-409940814eab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n",
      "x_train shape: (2000, 784)\n",
      "x_test shape: (2000, 784)\n"
     ]
    }
   ],
   "source": [
    "num_examples = 2000\n",
    "num_classes = 10\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "num_features = 784\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, y_train = x_train[:num_examples], y_train[:num_examples]\n",
    "x_test, y_test = x_test[:num_examples], y_test[:num_examples]\n",
    "x_train = x_train.reshape(num_examples, num_features)\n",
    "x_test = x_test.reshape(num_examples, num_features)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "train_labels_b = y_train\n",
    "test_labels_b = y_test\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.5759999752044678\n"
     ]
    }
   ],
   "source": [
    "## Model (feedforward model)\n",
    "model = Sequential() \n",
    "\n",
    "# Add a dense output layer where every node is connnected, and use softmax (pick the biggest mumber)\n",
    "model.add(Dense(10, input_shape=(num_features,), activation='softmax')) \n",
    "\n",
    "## Cost function & Objective (and solver)\n",
    "\n",
    "# Use stochastic gradient descent\n",
    "sgd = optimizers.SGD(lr=0.02)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train, train_labels_b, shuffle=False, batch_size=2000,verbose=0, epochs=52) \n",
    "score = model.evaluate(x_test, test_labels_b, verbose=0) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaAWvMVsT6dv"
   },
   "source": [
    "Exercise:  What do you expect to happen if we convert batch gradient descent to stochastic gradient descent?  Why?\n",
    "\n",
    "Let's try it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 56333,
     "status": "ok",
     "timestamp": 1602728375744,
     "user": {
      "displayName": "Todd Holloway",
      "photoUrl": "https://lh3.googleusercontent.com/-Fjjf5JgE-cQ/AAAAAAAAAAI/AAAAAAAARlo/rz8G-eBTNGo/s64/photo.jpg",
      "userId": "06017688276795782893"
     },
     "user_tz": 420
    },
    "id": "LWMIZ0t7T6dw",
    "outputId": "4216b8b4-a2da-497c-e7a3-852bae5e27f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8289999961853027\n"
     ]
    }
   ],
   "source": [
    "## Model\n",
    "model = Sequential() \n",
    "model.add(Dense(10, input_dim=num_features, activation='softmax')) \n",
    "\n",
    "## Cost function & Objective (and solver)\n",
    "sgd = optimizers.SGD(lr=0.02)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train, train_labels_b, shuffle=False, batch_size=1,verbose=0, epochs=50) \n",
    "score = model.evaluate(x_test, test_labels_b, verbose=0) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DRorgERT6dz"
   },
   "source": [
    "# PART 2: Multi-layer Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3pr45e0T6dz"
   },
   "source": [
    "---------\n",
    "\n",
    "Let's take our implementation of logistic regression (which recall is in fact a single layer neural network), and add a hidden layer, making it a two layer neural network.  Because we have a hidden layer, we will now train the model using backpropagation.\n",
    "\n",
    "Exercise: How do you expect this model to compare to KNN and logistic regression in terms of train time and accuracy?  Why?\n",
    "\n",
    "Let's try it out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7905,
     "status": "ok",
     "timestamp": 1602125989140,
     "user": {
      "displayName": "Todd Holloway",
      "photoUrl": "https://lh3.googleusercontent.com/-Fjjf5JgE-cQ/AAAAAAAAAAI/AAAAAAAARlo/rz8G-eBTNGo/s64/photo.jpg",
      "userId": "06017688276795782893"
     },
     "user_tz": 420
    },
    "id": "upwLDfSxT6d0",
    "outputId": "15cb1286-87a1-466e-e2e2-d803f50721a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.48643702268600464\n",
      "Test accuracy: 0.8479999899864197\n"
     ]
    }
   ],
   "source": [
    "## Model (feedforward) with 2 layers\n",
    "model = Sequential() \n",
    "\n",
    "# One hidden layer\n",
    "model.add(Dense(input_dim=num_features, units=20, activation='sigmoid')) \n",
    "\n",
    "# One output layer\n",
    "model.add(Dense(input_dim=20, units=10, activation='softmax')) \n",
    "\n",
    "## Cost function & Objective (and solver)\n",
    "sgd = optimizers.SGD(lr=0.02)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train, train_labels_b, shuffle=False, batch_size=10,verbose=0, epochs=50) \n",
    "score = model.evaluate(x_test, test_labels_b, verbose=0) \n",
    "print('Test score:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fqmYdFOT6d3"
   },
   "source": [
    "--------\n",
    "\n",
    "As interest in networks with more layers and more complicated architechures has increased, a couple of tricks have emerged and become standard practice.  Let's look at two of those--rectifier activation and dropout noise.\n",
    "\n",
    "Exercise:  We saw an improvement from adding a hidden layer.  What do you expect to happen if a second hidden layer was added?  \n",
    "\n",
    "Let's try it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8164,
     "status": "ok",
     "timestamp": 1602126068052,
     "user": {
      "displayName": "Todd Holloway",
      "photoUrl": "https://lh3.googleusercontent.com/-Fjjf5JgE-cQ/AAAAAAAAAAI/AAAAAAAARlo/rz8G-eBTNGo/s64/photo.jpg",
      "userId": "06017688276795782893"
     },
     "user_tz": 420
    },
    "id": "NcUZyxD7T6d4",
    "outputId": "47423005-e3dc-440f-8d5a-df770376d6e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.6939327120780945\n",
      "Test accuracy: 0.7925000190734863\n"
     ]
    }
   ],
   "source": [
    "## Model (3 layers)\n",
    "model = Sequential() \n",
    "\n",
    "#1st hidden layer\n",
    "model.add(Dense(units=20, input_dim=num_features, activation='sigmoid'))\n",
    "\n",
    "#2nd hidden layer\n",
    "model.add(Dense(units=20, input_dim=20, activation='sigmoid'))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(units=10, input_dim=20, activation='softmax')) \n",
    "\n",
    "## Cost function & Objective (and solver)\n",
    "sgd = optimizers.SGD(lr=0.02)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train, train_labels_b, shuffle=False, batch_size=10,verbose=0, epochs=50) \n",
    "score = model.evaluate(x_test, test_labels_b, verbose=0) \n",
    "print('Test score:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PKPYRlaT6d9"
   },
   "source": [
    "#### Activation Revisted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "584_rxgnT6d9"
   },
   "source": [
    "Let's look at a recent idea around activation closely associated with deep learning.  In 2010, in a paper published at NIPS (https://www.utc.fr/~bordesan/dokuwiki/_media/en/glorot10nipsworkshop.pdf), Yoshua Bengio showed that rectifier activation works better empirically than sigmoid activation when used in the hidden layers.  \n",
    "\n",
    "The rectifier activation is simple: f(x)=max(0,x).  Intuitively, the difference is that as a sigmoid activated node approaches 1 it stops learning even if error continues to be propagated to it, whereas the rectifier activated node continue to learn (at least in the positive direction).  Rectifiers also speed up training.\n",
    "\n",
    "Although the paper was published in 2010, the technique didn't gain widespread adoption until 2012 when members of Hinton's group spread the word, including with this Kaggle entry: http://blog.kaggle.com/2012/11/01/deep-learning-how-i-did-it-merck-1st-place-interview/\n",
    "\n",
    "Let's change the activation in our 2 layer network to rectifier and see what happens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7924,
     "status": "ok",
     "timestamp": 1602126117197,
     "user": {
      "displayName": "Todd Holloway",
      "photoUrl": "https://lh3.googleusercontent.com/-Fjjf5JgE-cQ/AAAAAAAAAAI/AAAAAAAARlo/rz8G-eBTNGo/s64/photo.jpg",
      "userId": "06017688276795782893"
     },
     "user_tz": 420
    },
    "id": "82Rc2Kw6T6d-",
    "outputId": "91e25750-37c9-4f17-f75d-293b4ed7517b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.5663594603538513\n",
      "Test accuracy: 0.8539999723434448\n"
     ]
    }
   ],
   "source": [
    "## Model\n",
    "model = Sequential() \n",
    "\n",
    "# To prevent overfitting, we can use relu\n",
    "model.add(Dense(units=30, input_dim=num_features, activation='relu')) \n",
    "model.add(Dense(units=10, input_dim=30, activation='softmax')) \n",
    "\n",
    "## Cost function & Objective (and solver)\n",
    "sgd = optimizers.SGD(lr=0.02)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train, train_labels_b, shuffle=False, batch_size=10,verbose=0, epochs=50) \n",
    "score = model.evaluate(x_test, test_labels_b, verbose=0) \n",
    "print('Test score:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkzWTevcT6eB"
   },
   "source": [
    "#### Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0fyd5ohT6eB"
   },
   "source": [
    "Previously when working with the MNIST data we saw a benefit in generalization from adding noise to the training data.  Let's try that again here, however this time with a trick for adding noise called 'Dropouts'.  The idea with dropouts is that instead of (or in addition to) adding noise to our inputs, we add noise by having each node return 0 with a certain probability during training.  This trick both improves generalization in large networks and speeds up training.\n",
    "\n",
    "Hinton introduced the idea in 2012 and gave an explanation of why it's similar to bagging (http://arxiv.org/pdf/1207.0580v1.pdf)\n",
    "\n",
    "Let's give it a try..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YkKhBp9fT6eC",
    "outputId": "d0e0c593-129c-4794-a578-499d12cea515"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.47055062651634216\n",
      "Test accuracy: 0.8539999723434448\n"
     ]
    }
   ],
   "source": [
    "## Model\n",
    "model = Sequential() \n",
    "model.add(Dense(units=30, input_dim=num_features, activation='relu')) \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=10, input_dim=30, activation='softmax')) \n",
    "\n",
    "## Cost function & Objective (and solver)\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train, train_labels_b, shuffle=False, batch_size=10,verbose=0, epochs=100) \n",
    "score = model.evaluate(x_test, test_labels_b, verbose=0) \n",
    "print('Test score:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
