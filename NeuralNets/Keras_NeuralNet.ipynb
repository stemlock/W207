{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Tensorflow version 2.4.1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "np.random.seed(0)\n",
    "print (\"OK\")\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5488135])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features = 784\n",
      "Train set = 2000\n",
      "Test set = 2000\n"
     ]
    }
   ],
   "source": [
    "# Repeating steps from Project 1 to prepare mnist dataset. \n",
    "X, Y = fetch_openml(name='mnist_784', return_X_y=True, cache=False)\n",
    "X = X / 255.0\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "numExamples = 2000\n",
    "test_data, test_labels = X[70000-numExamples:], Y[70000-numExamples:]\n",
    "train_data, train_labels = X[:numExamples], Y[:numExamples]\n",
    "numFeatures = train_data[1].size\n",
    "numTrainExamples = train_data.shape[0]\n",
    "numTestExamples = test_data.shape[0]\n",
    "print ('Features = %d' %(numFeatures))\n",
    "print ('Train set = %d' %(numTrainExamples))\n",
    "print ('Test set = %d' %(numTestExamples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes = 10\n"
     ]
    }
   ],
   "source": [
    "def binarizeY(data):\n",
    "    binarized_data = np.zeros((data.size, 10))\n",
    "    for j in range(0, data.size):\n",
    "        feature = data[j:j+1]\n",
    "        i = feature.astype(np.int64) \n",
    "        binarized_data[j, i] = 1\n",
    "    return binarized_data\n",
    "train_labels_b = binarizeY(train_labels)\n",
    "test_labels_b = binarizeY(test_labels)\n",
    "numClasses = train_labels_b[1].size\n",
    "print ('Classes = %d' %(numClasses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time = 0.06\n",
      "Accuracy = 0.9125\n",
      "Prediction time = 5.72\n"
     ]
    }
   ],
   "source": [
    "neighbors = 1\n",
    "knn = KNeighborsClassifier(neighbors)\n",
    "# we'll be waiting quite a while if we use 60K examples, so let's cut it down.  You may want to run the full 60K on your own later to see what the accuracy is.\n",
    "mini_train_data, mini_train_labels = X[:numExamples], Y[:numExamples] \n",
    "start_time = time.time()\n",
    "knn.fit(mini_train_data, mini_train_labels)\n",
    "print ('Train time = %.2f' %(time.time() - start_time))\n",
    "start_time = time.time()\n",
    "accuracy = knn.score(test_data, test_labels)\n",
    "print ('Accuracy = %.4f' %(accuracy))\n",
    "print ('Prediction time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time = 2.37\n",
      "Test score: 0.3939725160598755\n",
      "Test accuracy: 0.8845000267028809\n"
     ]
    }
   ],
   "source": [
    "## Model (feed-forward)\n",
    "model = Sequential() \n",
    "model.add(Dense(10, input_dim=784, activation='softmax')) \n",
    "# How many weights will we have...?\n",
    "\n",
    "## Cost function & Objective (and solver)\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.02)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "start_time = time.time()\n",
    "\n",
    "# Batch size is the number of samples per stochastic update, which is correlated with the number of models that\n",
    "# are trained. Epochs are the passes through the ENTIRE train data set irregardless of the batch size\n",
    "history = model.fit(train_data, train_labels_b, shuffle=False, batch_size=20, verbose=0, epochs=50) \n",
    "print ('Train time = %.2f' %(time.time() - start_time))\n",
    "score = model.evaluate(test_data, test_labels_b, verbose=0) \n",
    "print('Test score:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
